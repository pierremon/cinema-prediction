---
title: "R Notebook"
output: html_notebook
---


```{r}
###############Initialisation 
rm(list=objects())
###############packages
library(dygraphs)
library(xts)
library(tidyverse)
library(car)
library(dummies)
dataTrain <- read_delim("/media/matthieu/disque dur Matthieu/Matthieu/Mes Documents/R/Projet M2 StatML/tmdb-box-office-prediction/train.csv", col_names =TRUE, delim=',')
#dataTest <- read_delim("//mnt/chromeos/MyFiles/Mes Documents/Projet StatML/test.csv", col_names =TRUE, delim=',')
summary(dataTrain)
```

# ##########################
# I - 3 catégories de BDDS #
# ##########################



# 1. Catégorie 1
# --------------

```{r}
DataCat1 <- subset(dataTrain, select = c("budget", "popularity", "runtime", "revenue"))
DataCat2 <- subset(dataTrain, select = c("revenue"))
```




# 2. Catégorie 2
# --------------

```{r}
###############Conversion en données chiffrées
genres_matching_point <- "Comedy|Horror|Action|Drama|Documentary|Science Fiction|Crime|Fantasy|Thriller|Animation|Adventure|Mystery|War|Romance|Music|Family|Western|History|TV Movie|Foreign"    #inspiré du notebook sur internet
#dataTrain$genresName <- str_extract(dataTrain$genres, genres_matching_point)            #le pb c'est qu'on ne garde que le premier genres, parfois classé par ordre alphabétique ??
genresName <- str_extract_all(dataTrain$genres, genres_matching_point, simplify = TRUE)        #c'est pour ça que je l'ai changé en extract all, et la ca fait ce que je veux ie : une matrice qui a pour lignes chaque film (3000) donne le vecteur de tous les genres auquel il appartient
#creation d'une "dummy matrix" (une matrice avec des 1 sile film (ligne) appartient au genre (colonne), 0 sinon)
#étant donné qu'il y a 20 genres on va renvoyer une matrice avec 20 colonnes (une pour chaque genre) et 3000 lignes
dummyMatrixGenres <- matrix(0, ncol = 20, nrow = 3000)
genres_matching_point <- strsplit(genres_matching_point, "|", fixed= TRUE)
for (i in 1:length(genres_matching_point[[1]])) {
  for (j in (1:length(dataTrain$id))) {
    if (genres_matching_point[[1]][i] %in% genresName[j,]){
      dummyMatrixGenres[j,i] <- 1
    }
  }
}
#on va injecter cette dummy matrice dans notre data frame (et on met les bons noms de colonne)
GenresDataFrame <- as.data.frame(dummyMatrixGenres)
colnames(GenresDataFrame) = genres_matching_point[[1]]
DataCat2 <- data.frame(DataCat2, GenresDataFrame)
```

```{r}
ProductionCountry <- str_extract_all(string = dataTrain$production_countries, 
                                      pattern = "[:upper:]{2}", simplify = TRUE) #inspiré du notebook, sert surtout à avoir une colonne plus jolie
tableProductionCountry <- names(table(ProductionCountry, exclude = c("", "NA", NA)))
ncolumn = length(tableProductionCountry)
dummyMatrixProdCountry <- matrix(0, ncol = ncolumn, nrow = 3000)
for (i in 1:length(tableProductionCountry)) {
  for (j in (1:length(dataTrain$id))) {
    if (tableProductionCountry[i] %in% ProductionCountry[j,]){
      dummyMatrixProdCountry[j,i] <- 1
    }
  }
}
#on va injecter cette dummy matrice dans notre data frame (et on met les bons noms de colonne)
ProdCountryDataFrame <- as.data.frame(dummyMatrixProdCountry)
colnames(ProdCountryDataFrame) = tableProductionCountry
DataCat2 <- data.frame(DataCat2, ProdCountryDataFrame)
```

```{r}
#Dans homepages, on va mettre 1 si y'en a une, 0 si y'en a pas en gros.
dummyMatrixHomepages <- matrix(1,ncol=1,nrow=3000)
for (i in 1:3000){
  if (is.na(dataTrain$homepage[i])){
   dummyMatrixHomepages[i,1]=0 
  }
}
DataCat2 <- data.frame(DataCat2, dummyMatrixHomepages)
```

```{r}
#Une seule langue originale par film. 36 langues différentes. On crée 36 colonnes, et pour chaque ligne (i.e film), on a un 1 dans la colonne correspondant à sa langue originale.
dummyMatrixLO <- matrix(0,ncol=36,nrow=3000)
tableLO=names(table(dataTrain$original_language))
for (i in 1:36) {
  for (j in (1:3000)) {
    if (tableLO[i] %in% dataTrain$original_language[j]){
      dummyMatrixLO[j,i] <- 1
    }
  }
}
#Puis on injecte:
DataCat2 <- data.frame(DataCat2, dummyMatrixLO)
```


Il y a quelques colonnes que je ne prends pas en compte dans cat 2 :

belongs_to_collection : nom de la série dans laquelle s'inscrit le film (ex : James Bond), vide s'il n'y a rien
production_companies : (parfois vide)
release_date : date de sortie
keywords
cast : (acteurs + roles...)
crew : (réals...)




# ##########################################
# II - Divisons en data train et data test #
# ##########################################

```{r}
TrainCat1 <- DataCat1[1:2500,]
TestCat1 <- DataCat1[2501:3000,]

TrainCat2 <- DataCat2[1:2500,]
TestCat2 <- DataCat2[2501:3000,]
```





# ################################################
# III - Maintenant commençons vraiment les algos : Catégorie 1 #
# ################################################

On dispose donc ici des variables durée, popularité et budget du film, pour prédire son revenu. On effectue pour cela des modèles additifs:

```{r}

train<-DataCat2[1:2500,]


g<-gam(revenue~s(budget)+s(popularity)+s(runtime),data = train)
summary(g) 
```
funfact: quand on regarde les degrés de liberté, c'est 8, 9 et 3.5. Donc plus linéaire en runtime qu'en budget (marrant).

```{r}
test<-DataCat2[2500:3000,]

pred<-predict(g,newdata = test)
actual=dataTrainGros$revenue[2500:3000]

#R au carré:
rss <- sum((pred - mean(actual)) ^ 2)
tss <- sum((actual - pred) ^ 2)
rsq <- rss/(rss+tss)
rsq 
```
On obtient en gros 0.60-0.65 pour le R-carré. 

Ensuite qqs plots:

prédictions et vraies valeurs en fonction du budget:
```{r}

Budget<-test$budget
Revenue<-actual
y2<-pred
plot(Budget,Revenue,type="p")
points(Budget,y2,col=2)
legend("topleft", legend=c("Training values", "Testing values"),
       col=c("black", "red"), lty=1:2, cex=0.8, pch=c("o","o"))
```
en fonction de la popularité:
```{r}

popu<-test$popularity
Revenue<-actual
y2<-pred
plot(popu,Revenue,type="p")
points(popu,y2,col=2)
legend("topleft", legend=c("Training values", "Testing values"),
       col=c("black", "red"), lty=1:2, cex=0.8, pch=c("o","o"))
```
C'est un petit peu mions joli qu'en fonction du budget (a l'air moins linéaire quoi)

prédictions en fonction des vraies valeurs, c'est pas une ligne droite, mais ça a un peu de gueule je trouve:
```{r}
plot(actual,pred)
```



# ################################################
# IV - Les algos : Catégorie 2 #
# ################################################


# 1. XGBoost
# ----------

Commençons par appeler les bibliothèques
```{r}
library(xgboost)
library(caret)
```


Maintenant il s'agit de mettre la BDD dans le bon format.

```{r}
#on enlève revenue des BDDs
X_train = xgb.DMatrix(as.matrix(TrainCat2 %>% select(-revenue)))
y_train = TrainCat2$revenue
X_test = xgb.DMatrix(as.matrix(TestCat2 %>% select(-revenue)))
y_test = TestCat2$revenue
```

```{r}
xgb_trcontrol = trainControl(method = "cv", number = 5, allowParallel = TRUE, 
    verboseIter = FALSE, returnData = FALSE)
xgbGrid <- expand.grid(nrounds = c(100,200),  
                       max_depth = c(3, 5, 10, 15, 20),
                       colsample_bytree = seq(0.5, 0.9, length.out = 5),
                       ## valeurs par défaut : 
                       eta = 0.1,
                       gamma=0,
                       min_child_weight = 1,
                       subsample = 1
                      )

set.seed(0)   #est-ce vraiment utile ??
xgb_model = train(X_train, y_train, trControl = xgb_trcontrol, tuneGrid = xgbGrid, 
    method = "xgbTree")
```


Regardons les résultats :
```{r}
xgb_model$bestTune
```

```{r}
predicted = predict(xgb_model, X_test)
residuals = y_test - predicted
RMSE = sqrt(mean(residuals^2))

y_test_mean = mean(y_test)
tss = sum((y_test - y_test_mean)^2)
rss = sum(residuals^2)
rsq = 1 - (rss/tss)

print(RMSE)
print(rsq)
```

# ################################################
# V - Les algos : Catégorie 3 #
# ################################################

On dispose de tout sauf les 2 autres catégories, et on veut prédire l'erreur comise à la suite du traitement des 2 précédentes catégories.
Appelons Error3 cette erreur à prédire. 

```{r}
Error3<-
```
On va faire sélection de variables grâce au Lasso, puis appliquer les algos de la catégorie 2 (forêts et gradient boosting).

```{r}
train<-DataCat3[0:2500,]
test<-DataCat3[2501:3000,]

x_train=as.matrix(train[0:2500,][1000:89375])
y_train=as.matrix(Error3[0:2500])
```

```{r}
lasso_model_cv=cv.glmnet(x_train,y_train,alpha=1)
plot(lasso_model_cv)
```
Ce dernier plot montre la RMSE et le nombre de variables sélectionnées en fonction du lambda (pas mal comme plot).
En général le meilleur lambda sélectionne dans les 600 variables parmi 89k, et vaut dans les 10^4. On le sélectionne:

```{r}
numero_du_best_model=which(lasso_model_cv$lambda==lasso_model_cv$lambda.min)
Lasso=lasso_model_cv$glmnet.fit$beta[,numero_du_best_model]
print (log(numero_du_best_model))
```

Sélection des variables: il ne nous reste plus qu'à retraiter les données:
```{r}
newlasso=c()
varsel=c() #listes vides à remplir, newlasso n'aura que les variables sélectionnées, et varsel coderal ces variables sélectionnées
n=0 #sera le nb de variables sélectionnées

for (i in Lasso) {
  if(i==0.) {   #si la variable n'est pas sélectionnée, on met 0 dans varsel
    varsel <- append(varsel,c(0))
  }
  
  else{ #si variable sélectionnée, on met 1 dans varsel
    newlasso <- append(newlasso,c(i))
    varsel<-append(varsel,c(1))
    n<-n+1
  }
}
print(n) #le nb de variables sélectionnées
```{r}

Maintenant on crée les données avec que les variables "importantes":
```{r}
k=1
var=as.list(Lasso)
for (i in varsel) {
  if(i==0){
    var<-var[-k]  #en gros on supprime de Lasso les variables non sélectionnées.
    k<-k-1
  }
  k<-k+1
  
}

x<-DataCat3
x2<-subset(x, select = c(names(var))) #on a pris toutes les variables (pour les 3000 films) sélectionnées
```


Puis juste faire le travail de 2 avec x2 pour prédire Error3.




